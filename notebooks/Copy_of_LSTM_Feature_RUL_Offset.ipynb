{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Q0lCX-TpktVi",
   "metadata": {
    "id": "Q0lCX-TpktVi"
   },
   "source": [
    "## Setup Drive and Data Folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wtgGQz6fFfCt",
   "metadata": {
    "id": "wtgGQz6fFfCt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "PRECISION = 4 # 3 of digits to keep after the decimal point\n",
    "\n",
    "RUNNING_ON_COLAB = True # we assume running on CoLab! Change to False if running locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "U2tjHc5Tjy-l",
   "metadata": {
    "id": "U2tjHc5Tjy-l"
   },
   "source": [
    "*italicized text*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KSmhhEgmF-s3",
   "metadata": {
    "id": "KSmhhEgmF-s3"
   },
   "outputs": [],
   "source": [
    "## mount gdrive\n",
    "! pip install google.colab\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1IzYaftQFfCw",
   "metadata": {
    "id": "1IzYaftQFfCw"
   },
   "outputs": [],
   "source": [
    "# If running locally, define current working path\n",
    "path = os.getcwd()\n",
    "\n",
    "# If Google colab\n",
    "if RUNNING_ON_COLAB:\n",
    "  path = \"/content/gdrive/My Drive/Colab Notebooks/RandomForest_FeatureData\"\n",
    "\n",
    "print(path)\n",
    "\n",
    "# define current data path. This is after we did classification. We have done some\n",
    "# cleaning already.\n",
    "data_path = path + '/results'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WXeEz0OMFfC3",
   "metadata": {
    "id": "WXeEz0OMFfC3"
   },
   "source": [
    "## Util - add RUL column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zu3YROgaJe7V",
   "metadata": {
    "id": "zu3YROgaJe7V"
   },
   "outputs": [],
   "source": [
    "csv_file = data_path + '/combined_offset_misalignment.csv'\n",
    "df_temp = pd.read_csv(csv_file, chunksize=50000) \n",
    "df = pd.concat(df_temp, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "VZl9GolRFfC5",
   "metadata": {
    "id": "VZl9GolRFfC5",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Let's find the youngest & oldest timestamp\n",
    "\n",
    "df['wf_start_time'] = pd.to_datetime(df['wf_start_time']) # make sure it is datetime\n",
    "\n",
    "youngest = min(df.wf_start_time)\n",
    "oldest = max(df.wf_start_time)\n",
    "print(youngest)\n",
    "print(oldest)\n",
    "span = oldest - youngest\n",
    "print(span)\n",
    "print(span.total_seconds())\n",
    "\n",
    "## Using Oldest - current to determine the RUL\n",
    "df['rul'] = df['wf_start_time'].apply(lambda x: (oldest - x).total_seconds())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "TFE0gSOWDwzC",
   "metadata": {
    "id": "TFE0gSOWDwzC"
   },
   "outputs": [],
   "source": [
    "# save back with RUL\n",
    "df.to_csv(path + '/results/combined_offset_misalignment_with_RUL.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mZlQI-jMFfC6",
   "metadata": {
    "id": "mZlQI-jMFfC6"
   },
   "source": [
    "## LSTM Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gPlWZSNwFfC6",
   "metadata": {
    "id": "gPlWZSNwFfC6"
   },
   "outputs": [],
   "source": [
    "## Common imports\n",
    "colab = True\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csv\n",
    "import os\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import f1_score, cohen_kappa_score\n",
    "\n",
    "from numpy import array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Input\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "# from pandas_ml import ConfusionMatrix\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.available\n",
    "%matplotlib inline\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "RNDSEED = np.random.seed(39)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oCOjbuyRFfC6",
   "metadata": {
    "id": "oCOjbuyRFfC6"
   },
   "source": [
    "### Exploring the data a bit & prep the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eLdqWDopnBOC",
   "metadata": {
    "id": "eLdqWDopnBOC"
   },
   "outputs": [],
   "source": [
    "## future, let's read the file\n",
    "if colab == False:\n",
    "  csv_file = \"C:\\\\Users\\\\colte\\\\Downloads\\\\combined_offset_misalignment_with_RUL.csv\"\n",
    "else:\n",
    "  csv_file = path + '/results/combined_offset_misalignment_with_RUL.csv'\n",
    "df_temp = pd.read_csv(csv_file, chunksize=50000) \n",
    "big_df = pd.concat(df_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RLIt0IQxFfC6",
   "metadata": {
    "id": "RLIt0IQxFfC6"
   },
   "outputs": [],
   "source": [
    "df = big_df # reset \n",
    "\n",
    "# drop unwanted cols\n",
    "df = df[df.columns.drop(list(df.filter(regex='Unnamed')))] # drop Unnamed\n",
    "df = df[df.columns.drop(list(df.filter(regex='wf_start_time')))] # drop time column\n",
    "df = df[df.columns.drop(list(df.filter(regex='status')))] # drop status column\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wv69I-w9GhxA",
   "metadata": {
    "id": "Wv69I-w9GhxA"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SatLion8GqqX",
   "metadata": {
    "id": "SatLion8GqqX"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EzbhobY4FfC7",
   "metadata": {
    "id": "EzbhobY4FfC7"
   },
   "source": [
    "### RF works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HCg0_Tb7FfC7",
   "metadata": {
    "id": "HCg0_Tb7FfC7"
   },
   "outputs": [],
   "source": [
    "# Get X & y\n",
    "# Naming convention: X as predictors; y as response.\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Set the sequence length and batch size\n",
    "seq_length = 100\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "y = df['rul'] # pop response\n",
    "unscaled_X = df.drop('rul',axis = 1) # drop response\n",
    "X = scaler.fit_transform(unscaled_X)\n",
    "X_scaled_with_y = pd.concat([pd.DataFrame(X, columns=unscaled_X.columns), y], axis=1)\n",
    "\n",
    "print(X)\n",
    "\n",
    "# Create the TimeseriesGenerator object\n",
    "data_generator = TimeseriesGenerator(data=X_scaled_with_y.drop('rul', axis=1).values,\n",
    "                                      targets=X_scaled_with_y['rul'].values,\n",
    "                                      length=seq_length,\n",
    "                                      batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "# print(len(df))\n",
    "# Extract the data and targets from the generator\n",
    "x_train = []\n",
    "y_train = []\n",
    "for i in range(len(data_generator)):\n",
    "    x, y = data_generator[i]\n",
    "    x_train.append(x)\n",
    "    y_train.append(y)\n",
    "X_train_seq = np.concatenate(x_train)\n",
    "y_train_seq = np.concatenate(y_train)\n",
    "\n",
    "# Print the shape of the input and target data\n",
    "print('Input data shape:', X_train_seq.shape)\n",
    "print('Target data shape:', y_train_seq.shape)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print (X.shape)\n",
    "# print (X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lEm5NkpYFfC7",
   "metadata": {
    "id": "lEm5NkpYFfC7"
   },
   "outputs": [],
   "source": [
    "## true orignal ones\n",
    "\n",
    "\n",
    "# data split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_seq, y_train_seq, random_state=RNDSEED)\n",
    "\n",
    "print(y_train.shape[0], y_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "PH81YLbzFfC7",
   "metadata": {
    "id": "PH81YLbzFfC7"
   },
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "BSge05iDFfC8",
   "metadata": {
    "id": "BSge05iDFfC8",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(X_train.dtypes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wFNrHFG6FfC8",
   "metadata": {
    "id": "wFNrHFG6FfC8"
   },
   "outputs": [],
   "source": [
    "y_train.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SFqzSXDoFfC8",
   "metadata": {
    "id": "SFqzSXDoFfC8"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.regularizers import l2\n",
    "\n",
    "\n",
    "# n_steps = seq_length\n",
    "# n_features = 46\n",
    "# n_samples = len(X_test)\n",
    "\n",
    "# X_test1 = X_test.to_numpy()\n",
    "\n",
    "# X_test2 = X_test.reshape(n_samples, n_steps, n_features)\n",
    "\n",
    "# X_train1 = X_train.to_numpy()\n",
    "# print(X_train)\n",
    "\n",
    "def LSTM_RUL():\n",
    "  n_steps = seq_length\n",
    "  n_features = 46\n",
    "  n_samples = len(X_train)\n",
    "\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(256, activation='tanh', input_shape = (n_steps, n_features)))\n",
    "  model.add(Dense(units=1, activation='relu'))\n",
    "  # model.add(LSTM(64, activation='relu', input_shape = (n_steps, n_features)))\n",
    "\n",
    "  # model.add(Dense(units=1, activation='sigmoid'))\n",
    "  model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001))\n",
    "\n",
    "  # print(len(X_train1))\n",
    "\n",
    "  # X_train2 = X_train.reshape(n_samples, n_steps, n_features)\n",
    "  # print(X_train2)\n",
    "\n",
    "  # fit model\n",
    "\n",
    "  early_stopping = EarlyStopping(monitor='loss', patience=5, verbose=1, mode='min')\n",
    "\n",
    "  history = model.fit(X_train, y_train, epochs=50, callbacks=[early_stopping])\n",
    "\n",
    "  loss_history = history.history['loss']\n",
    "\n",
    "  # Plot the loss history\n",
    "  plt.plot(np.arange(len(loss_history)), loss_history, label='Training Loss')\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "  return model\n",
    "\n",
    "L = LSTM_RUL()\n",
    "\n",
    "\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = L.predict(X_test)\n",
    "\n",
    "# Calculate R-squared\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "print(\"R2 score:\", r2)\n",
    "\n",
    "loss = L.evaluate(X_test, y_test)\n",
    "print(\"Test loss:\", loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bt-Tb3eJpj3M",
   "metadata": {
    "id": "bt-Tb3eJpj3M"
   },
   "source": [
    "### Save/Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pO_ahHURoV-V",
   "metadata": {
    "id": "pO_ahHURoV-V"
   },
   "outputs": [],
   "source": [
    "## utility - save/load the model\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# to save\n",
    "# joblib.dump(rf, path + '/results/random_forest_offset_RUL.joblib') \n",
    "\n",
    "# to load\n",
    "rf = joblib.load(path + '/results/random_forest_offset_RUL.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "NhrnHD6IKlo0",
   "metadata": {
    "id": "NhrnHD6IKlo0"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zKFQczsZKoYD",
   "metadata": {
    "id": "zKFQczsZKoYD"
   },
   "outputs": [],
   "source": [
    "y_pred = rf.predict(X_test) ## using the untinted dataset!\n",
    "    \n",
    "print('R^2:', metrics.r2_score(y_test, y_pred))\n",
    "print('Mean Absolute Error (MAE):', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error (MSE):', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Mean Absolute Percentage Error (MAPE):', metrics.mean_absolute_percentage_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error (RMSE):', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) # np.sqrt\n",
    "\n",
    "print('Explained Variance Score:', metrics.explained_variance_score(y_test, y_pred))\n",
    "print('Max Error:', metrics.max_error(y_test, y_pred))\n",
    "print('Mean Squared Log Error:', metrics.mean_squared_log_error(y_test, y_pred))\n",
    "print('Median Absolute Error:', metrics.median_absolute_error(y_test, y_pred))\n",
    "\n",
    "## with n_estimators = 150\n",
    "\n",
    "# R^2: 0.9991654290573937\n",
    "# Mean Absolute Error (MAE): 1158.6216816072117\n",
    "# Mean Squared Error (MSE): 333818221.94037396\n",
    "# Root Mean Squared Error (RMSE): 18270.692979205083\n",
    "# Explained Variance Score: 0.9991654316522736\n",
    "# Max Error: 1998236.0866666667\n",
    "# Mean Squared Log Error: 0.012717975468372306\n",
    "# Median Absolute Error: 147.4266666667536\n",
    "\n",
    "## n_estimators = 10\n",
    "# R^2: 0.9990822069686238\n",
    "# Mean Absolute Error (MAE): 1213.5715068447246\n",
    "# Mean Squared Error (MSE): 367106044.79767376\n",
    "# Mean Absolute Percentage Error (MAPE): 1.0690696275079867e+17\n",
    "# Root Mean Squared Error (RMSE): 19160.011607451437\n",
    "# Explained Variance Score: 0.9990822141398445\n",
    "# Max Error: 1998818.5\n",
    "# Mean Squared Log Error: 0.012296657672986336\n",
    "# Median Absolute Error: 137.60000000009313"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kX7FUQOCPF10",
   "metadata": {
    "id": "kX7FUQOCPF10"
   },
   "source": [
    "### Show feature importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "y5Bjfug3FfC8",
   "metadata": {
    "id": "y5Bjfug3FfC8"
   },
   "outputs": [],
   "source": [
    "# RF: Get feature list\n",
    "## Learn more: https://towardsdatascience.com/explaining-feature-importance-by-example-of-a-random-forest-d9166011959e\n",
    "\n",
    "features = list(X_test.columns)  \n",
    "\n",
    "rf_imp_features = []\n",
    "\n",
    "## Plot the feature importance\n",
    "def plot_feature_importance ():\n",
    "    importances = rf.feature_importances_\n",
    "    \n",
    "    indices = np.argsort(importances)[len(importances)-25:] ## top 25    \n",
    "\n",
    "    plt.figure(figsize = (12,8))\n",
    "    plt.rcParams['font.size'] = 12\n",
    "    \n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "\n",
    "    features_y = []\n",
    "    for x in indices:\n",
    "        features_y.append(features[x])\n",
    "        \n",
    "    plt.yticks(range(len(indices)), features_y) \n",
    "\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.savefig(path + '/results/fi_offset_RUL.png') # save\n",
    "\n",
    "    plt.show()\n",
    "    return features_y\n",
    "\n",
    "rf_imp_features = plot_feature_importance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oqx5XNxWJ_L2",
   "metadata": {
    "id": "oqx5XNxWJ_L2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "kRTeH4HIJ_sq",
   "metadata": {
    "id": "kRTeH4HIJ_sq"
   },
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QlzciIeNb10e",
   "metadata": {
    "id": "QlzciIeNb10e"
   },
   "outputs": [],
   "source": [
    "# get a random sample to verify the results!\n",
    "## \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "RNDSEED = np.random.seed(39)\n",
    "\n",
    "dfs = df.sample(1000, random_state = RNDSEED) # data points\n",
    "# print(dfs.rul)\n",
    "X_dfs = dfs.drop('rul',axis = 1) # drop response\n",
    "\n",
    "ys_pred = rf.predict(X_dfs)\n",
    "# print(ys_pred)\n",
    "\n",
    "rul = dfs[['rul']]\n",
    "rul = rul.rename(columns={'rul': 'original'})\n",
    "rul['prediction'] = ys_pred\n",
    "# rul.head(5)\n",
    "\n",
    "matplotlib.style.use('ggplot') ## styling\n",
    "\n",
    "rul.plot.scatter(x='original', y='prediction', figsize=(16, 9), c='original', colormap='viridis') ## scatter\n",
    "\n",
    "plt.savefig(path + '/results/offset_rul_prediction.png') # save\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r2QDeKY2OMyC",
   "metadata": {
    "id": "r2QDeKY2OMyC"
   },
   "outputs": [],
   "source": [
    "## Plot the % of deviation \n",
    "\n",
    "rul['difference'] = (rul['prediction'] - rul['original']) /  rul['original']\n",
    "\n",
    "rul.index = range(len(rul.index)) ## reset index\n",
    "\n",
    "ax = rul.plot( y=[\"difference\"], figsize=(16, 9))\n",
    "\n",
    "ax.set_xlabel(\"Index\")\n",
    "ax.set_ylabel(\"Difference\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# out of couriosity \n",
    "print(len(rul[(rul['difference'] > 0.2) | (rul['difference'] < -0.2)])) # how many more than 20%? # 8 only!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Wv2mf1t5Lvf4",
   "metadata": {
    "id": "Wv2mf1t5Lvf4"
   },
   "outputs": [],
   "source": [
    "## styles\n",
    "plt.style.available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239y42NnRGYO",
   "metadata": {
    "id": "239y42NnRGYO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true,
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
