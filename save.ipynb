{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install plotly\n",
    "random_state=42\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dropout,Conv1D,MaxPooling1D,Flatten, Activation, Dense\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.layers import Dropout,Conv1D,MaxPooling1D,Flatten, Activation\n",
    "from tensorflow.keras.regularizers import L1L2\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "# from autosklearn.regression import AutoSklearnRegressor\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.regularizers import L2,L1,L1L2\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "import calendar\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, LSTM, Bidirectional, Dropout\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, auc, classification_report\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    mean_absolute_error,\n",
    "    explained_variance_score,\n",
    "    accuracy_score,\n",
    "    r2_score,\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    recall_score,\n",
    "    cohen_kappa_score,\n",
    "    matthews_corrcoef,\n",
    "    precision_recall_fscore_support,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold, GroupKFold, LeaveOneGroupOut\n",
    "import joblib\n",
    "from sklearn.metrics import roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "#     RocCurveDisplay.from_estimator(model, X_vals, y_vals)\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "from sklearn.metrics import f1_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%load_ext tensorboard\n",
    "%reload_ext tensorboard\n",
    "\n",
    "\n",
    "def sk_agregated_score(model, X_test, y_test,threshold, ratio_threshold):\n",
    "    #For testing, input whole test at a time\n",
    "    # threshold: threshold for individual predictions\n",
    "    # ratio_threshold: ratio of positive to negative predictions required\n",
    "\n",
    "    Q_y_test=[]\n",
    "    Q_y_pred=[]\n",
    "    Q_stoped_window=[]\n",
    "    Q_total_windows=[]\n",
    "    Q_test_name=[]\n",
    "    for p,x in enumerate(zip(metadata[metadata['Type']==dataframe_type].iloc[:,2],metadata[metadata['Type']==dataframe_type].iloc[:,3],metadata[metadata['Type']==dataframe_type].iloc[:,1])):\n",
    "        try:\n",
    "            _X_test=X_test[x[0]:x[1]] #ONLY ONE TEST\n",
    "            _y_test=y_test[x[0]:x[1]]#ONLY ONE TEST\n",
    "\n",
    "            test_name=x[2]#metadata[metadata['Type']==dataframe_type].iloc[x,2] #1 String\n",
    "\n",
    "            # count of positive and negative predictions for each data point\n",
    "            pos_counts = 0    \n",
    "            neg_counts = 0\n",
    "            ratio=1\n",
    "\n",
    "            total_windows=len(_y_test)\n",
    "\n",
    "            Q_y_test.append(_y_test[0])\n",
    "            Q_total_windows.append(total_windows)\n",
    "            Q_test_name.append(test_name)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            for j in range(0,_y_test.shape[0],1):\n",
    "#                     print(\"!!!!!!!!!!!PREVIOUSCHECKPOINT!!!!!!!!!!!!!!!\")\n",
    "                prob = model.predict(_X_test[j].reshape(1,_X_test[j].shape[0] ),verbose=0)\n",
    "\n",
    "                prob=prob[0][0]\n",
    "\n",
    "                if prob >= threshold:\n",
    "\n",
    "                    pos_counts += 1\n",
    "                else:\n",
    "                    neg_counts += 1\n",
    "\n",
    "\n",
    "                if neg_counts == 0:\n",
    "                    ratio = float('inf') # set the ratio to infinity if there are no negative counts\n",
    "                else:\n",
    "                    ratio = pos_counts / neg_counts\n",
    "\n",
    "                # classify the entire sample as class 1 and move on to the next sample\n",
    "                if ratio >= ratio_threshold and ratio != float('inf'):\n",
    "\n",
    "                    y_pred = 1\n",
    "                    stoped_window=j\n",
    "\n",
    "                    break     \n",
    "\n",
    "\n",
    "                else:\n",
    "\n",
    "                    y_pred = 0\n",
    "                    stoped_window=j\n",
    "\n",
    "            Q_y_pred.append(y_pred)\n",
    "            if y_pred==1:\n",
    "\n",
    "                print(\"Stopped at: \",stoped_window)\n",
    "            else:\n",
    "                print(\"No Chatter Detected\")\n",
    "            Q_stoped_window.append(stoped_window)\n",
    "\n",
    "        except Exception as e:\n",
    "            pass\n",
    "    return Q_y_test,Q_y_pred,Q_stoped_window,Q_total_windows,Q_test_name\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def sk_agregated_testing_score(model,test_inputs,test_out):\n",
    "    pass\n",
    "\n",
    "\n",
    "#SEQUENCE GENERATOR\n",
    "\n",
    "def gen_seq(data, seq_length, columns):\n",
    "    L=[]\n",
    "    n = data.shape[0]\n",
    "    for i in range(0, n - seq_length,JUMPING_STEP):\n",
    "        try:\n",
    "            L.append(data[i:i+seq_length][columns].values)\n",
    "            if data[i:i+seq_length][columns].values.shape[0] != 100:\n",
    "                print(data[i:i+seq_length][columns].values.shape)\n",
    "        except:\n",
    "\n",
    "            break\n",
    "\n",
    "    return (L)\n",
    "\n",
    "\n",
    "#OUTPUTS\n",
    "def gen_labels(id_df, seq_length, label):\n",
    "    data_array = id_df[label].values\n",
    "    num_elements = data_array.shape[0]\n",
    "    return data_array[seq_length:num_elements:JUMPING_STEP]\n",
    "\n",
    "\n",
    "# evaluate a given model using cross-validation\n",
    "\n",
    "def cross_validation_function(selected_features,utc_time,random_state):\n",
    "    \n",
    "    #THIS DATA JUST RECEIVED HAS TO BECOME THE SEQUENCES\n",
    "    #WE UNITE THE SEQUENCES AGAIN LET THE K FOLD HANDLE THE SPLIT\n",
    "    \n",
    "\n",
    "    \n",
    "    data_x = selected_features.iloc[:,2:-3]\n",
    "    data_y = selected_features.iloc[:,-4:-2]\n",
    "    \n",
    "    \n",
    "#     print(training_x.tail())\n",
    "#     print(training_y.tail())\n",
    "#     input()\n",
    "\n",
    "\n",
    "    gkf = GroupKFold(n_splits=FOLDS[0])\n",
    "    groups= selected_features['Filename']\n",
    "    lst_accu_stratified = []\n",
    "    \n",
    "\n",
    "    for train_index, test_index in gkf.split(data_x,data_y,groups):\n",
    "        print(\"IN\")\n",
    "        _X_train, X_test = data_x.iloc[train_index], data_x.iloc[test_index]\n",
    "        print(\"SECOND\")\n",
    "        _y_train, y_test = data_y.iloc[train_index], data_y.iloc[test_index]\n",
    "        \n",
    "        \n",
    "        #I GET MY DATASETS SEPARATED THEN I CONSTRUCT THE SEQUENCES:\n",
    "        ###SECOND PART!1!!\n",
    "        #TODO: PERFORM THE SPLIT WITH 5 KFOLD! OVER THE TRINING DATA PREVIOSLY SPLITTED\n",
    "        #NOW WE PERFORM THE SPLITS FROM OUTSIDE\n",
    "        \n",
    "        \n",
    "        #TO CrEAtE thE VALIDATION SPLIT\n",
    "        splitter = GroupShuffleSplit(\n",
    "        test_size=.10,\n",
    "        n_splits=2,\n",
    "        random_state=random_state)\n",
    "        split = splitter.split(_X_train,_y_train, groups=_y_train['Filename'])\n",
    "        train_inds, vals_inds = next(split)\n",
    "\n",
    "        X_train,X_vals = _X_train.iloc[train_inds], _X_train.iloc[vals_inds]#CHANGED JT\n",
    "        y_train,y_vals = _y_train.iloc[train_inds], _y_train.iloc[vals_inds]#CHANGED JT\n",
    "        \n",
    "#         print(X_train.shape)\n",
    "#         print(X_train.head())\n",
    "#         input()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        #THIS TRAIN WILL HAVE A SUFFLED SEQUENCE BASED SPLIT FOR THE INNNER VALIDATION IN THE MODEL!\n",
    "        train_inputs = np.vstack([gen_seq(X_train[X_train['Filename'] == id], seq_length, [\"1-1: Channel 1 (Pa)\",\"Filename\"])\n",
    "            for id in X_train['Filename'].unique()])\n",
    "\n",
    "\n",
    "        test_inputs = np.vstack([gen_seq(X_test[X_test['Filename'] == id], seq_length, [\"1-1: Channel 1 (Pa)\",\"Filename\"])\n",
    "                            for id in X_test['Filename'].unique()])\n",
    "\n",
    "\n",
    "        #THIS ONE GOES FOR SK_AGGREGATION\n",
    "        vals_inputs = np.vstack([gen_seq(X_vals[X_vals['Filename'] == id], seq_length, [\"1-1: Channel 1 (Pa)\",\"Filename\"])\n",
    "                            for id in X_vals['Filename'].unique()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        train_out = np.vstack([gen_labels(y_train[y_train['Filename'] == id], seq_length, [\"IS_UNSTABLE\"] )\n",
    "                            for id in y_train['Filename'].unique()])\n",
    "\n",
    "        test_out = np.vstack([gen_labels(y_test[y_test['Filename'] == id], seq_length, [\"IS_UNSTABLE\"])\n",
    "                            for id in y_test['Filename'].unique()])\n",
    "\n",
    "        vals_out = np.vstack([gen_labels(y_vals[y_vals['Filename'] == id], seq_length, [\"IS_UNSTABLE\"])\n",
    "                            for id in y_vals['Filename'].unique()])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #NEW SPLITTING!!!   WE KEEP WHOLE SEQUENCES SHUFFLE SEQUENCES\n",
    "\n",
    "        # set aside 20% of train and test data for evaluation HERE WE SHUFFLE OUR SEQUENCES\n",
    "#         train_inputs, test_inputs, train_out, test_out = train_test_split(train_inputs, train_out,\n",
    "#             test_size=0.25, shuffle = True, random_state = random_state)\n",
    "\n",
    "    #     # Use the same function above for the validation set WE JUST SPLIT IT IN 0.25 and 0.75 OF THE PREVIOUS SPLIT\n",
    "        train_inputs, vals_inputs_inner, train_out, vals_out_inner = train_test_split(train_inputs, train_out, \n",
    "            test_size=0.25,shuffle=True, random_state= random_state) # 0.25 x 0.8 = 0.2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #DROPPING FILENAME ONLY FOR WHAT I WILL USE NOW\n",
    "\n",
    "        print(train_inputs.shape)\n",
    "        train_inputs=train_inputs[:,:,:-1]\n",
    "        vals_inputs_inner=vals_inputs_inner[:,:,:-1]\n",
    "        print(train_inputs.shape)\n",
    "\n",
    "\n",
    "\n",
    "        print(\"train_out\")\n",
    "        print(train_out.shape)\n",
    "        scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "        train_out=scaler.fit_transform(train_out)\n",
    "        train_out=train_out.reshape(-1,1)\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        print(\"vals_out_inner\")\n",
    "        print(vals_out_inner.shape)\n",
    "        scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "        vals_out_inner=scaler.fit_transform(vals_out_inner)\n",
    "        vals_out_inner=vals_out_inner.reshape(-1,1)\n",
    "\n",
    "        \n",
    "        \n",
    "        train_inputs=train_inputs.astype('float32')\n",
    "        vals_inputs_inner=vals_inputs_inner.astype('float32')\n",
    "        \n",
    "        train_out=train_out.astype('float32')\n",
    "\n",
    "\n",
    "        vals_out_inner=vals_out_inner.astype('float32')\n",
    "        \n",
    "\n",
    "        print(\"train_inputs:\",train_inputs.shape)\n",
    "\n",
    "        print(\"vals_inputs_inner:\",vals_inputs_inner.shape)\n",
    "        \n",
    "        print(\"train_out:\",train_out.shape)\n",
    "\n",
    "        print(\"vals_out_inner:\",vals_out_inner.shape)\n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "        path='/home/jupyter/DataProject'\n",
    "\n",
    "        log_dir = \"logs/fit/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "        print(log_dir)\n",
    "        tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        conf = tf.compat.v1.ConfigProto()\n",
    "        conf.gpu_options.allow_growth=True\n",
    "        session = tf.compat.v1.Session(config=conf)\n",
    "\n",
    "\n",
    "        nb_features = train_inputs.shape[2]\n",
    "        sequence_length  = train_inputs.shape[1]\n",
    "        nb_out = train_out.shape[1]\n",
    "\n",
    "        model = tf.keras.models.Sequential([\n",
    "            tf.keras.layers.Conv1D(filters=32,\n",
    "                       kernel_size=8,\n",
    "                       strides=1,\n",
    "                       activation='relu',\n",
    "                       padding='same'),\n",
    "            tf.keras.layers.LSTM(64, input_shape = (sequence_length, nb_features), return_sequences = True),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.LSTM(32),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(nb_out, activation = 'sigmoid')\n",
    "        ])\n",
    "\n",
    "        lr = tf.keras.callbacks.LearningRateScheduler(lambda epoch: 10**-7 * 10**(epoch/3))\n",
    "\n",
    "        model.compile(loss=tf.keras.losses.Huber(), optimizer = tf.keras.optimizers.Adam(lr = 10**-7), metrics =['mse','mae'])\n",
    "\n",
    "        es = EarlyStopping(monitor=\"val_loss\",\n",
    "                mode=\"auto\",\n",
    "                verbose=2,\n",
    "                patience=20,\n",
    "                min_delta=0.0001,\n",
    "                restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        history=model.fit(train_inputs,train_out,epochs=100,validation_data= (vals_inputs_inner,vals_out_inner) ,verbose=2,callbacks=[tensorboard_callback,lr,es])\n",
    "\n",
    "        \n",
    "\n",
    "        #THIS FUNCTION SAVES THE BEST THRESHOLD RATION POSSIBLE! RETIREVES THE BEST MODEL AS MODEL AND ALSO SAVES IT! AND METRICS AND BEST PARAMETERS\n",
    "        #THIS IS PER CV IN THE K-FOLD\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        _Q_y_test,_Q_y_pred,_Q_stoped_window,_Q_total_windows,_Q_test_name=sk_agregated_score(model, vals_inputs, vals_out, THRESHOLD, RATIO_THRESHOLD)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        #NOW TESTING SCORE\n",
    "        #THIS DELETES THE FILENAME THEN DEACTIVATE\n",
    "        #vals_inputs=vals_inputs[:,:,:-1]\n",
    "        \n",
    "        print(\"vals_out\")\n",
    "        print(vals_out.shape)\n",
    "        scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "        vals_out=scaler.fit_transform(vals_out)\n",
    "        vals_out=vals_out.reshape(-1,1)\n",
    "        \n",
    "        vals_inputs=vals_inputs.astype('float32')\n",
    "        vals_out=vals_out.astype('float32')\n",
    "        print(\"vals_inputs:\",vals_inputs.shape)\n",
    "        print(\"vals_out:\",vals_out.shape)\n",
    "        \n",
    "        \n",
    "        #THIS DELETES THE FILENAME THEN DEACTIVATE\n",
    "        #test_inputs=test_inputs[:,:,:-1]\n",
    "        \n",
    "        print(\"test_out\")\n",
    "        print(test_out.shape)\n",
    "        scaler=MinMaxScaler(feature_range=(0, 1))\n",
    "        test_out=scaler.fit_transform(test_out)\n",
    "        test_out=test_out.reshape(-1,1)\n",
    "\n",
    "        test_inputs=test_inputs.astype('float32')\n",
    "        test_out=test_out.astype('float32')\n",
    "        print(\"test_inputs:\",test_inputs.shape)\n",
    "        print(\"test_out:\",test_out.shape)\n",
    "        \n",
    "        \n",
    "        sk_agregated_testing_score(model,test_inputs,test_out) #TO BE IMPLEMENTES IT WILL USE THE TESTING PORTION AND THE MODEL TO PERFORM A TESTING\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        lst_accu_stratified.append(\"CV ROUND\")\n",
    "\n",
    "    return lst_accu_stratified\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(FOLDS, THRESHOLD,RATIO_THRESHOLD,PREFIXES,random_state):\n",
    "    print(\"###########################NEW_EXPERIMENT_STARTED#############################\")\n",
    "    date = datetime.utcnow()\n",
    "    utc_time = calendar.timegm(date.utctimetuple())\n",
    "    print(\"UTC TIME: \",utc_time)\n",
    "    \n",
    "\n",
    "    data = pd.read_csv(\"transformed_data/transformed_0_final_dataset.csv\",index_col=False)\n",
    "    data=data[data['Filename'].str.contains('data/Demo Tool Makino/') ]\n",
    "\n",
    "\n",
    "    data['Points'] = range(0,len(data))\n",
    "    sorted_data = data.sort_values([\"Filename\",'Points'])\n",
    "    samples_list = list(sorted_data[\"Filename\"].unique())\n",
    "    prefixes = PREFIXES\n",
    "    data=data[data.Filename.str.startswith(tuple(prefixes))]\n",
    "\n",
    "    ### Preprocessing The Features\n",
    "    data['Points'] = range(0,len(data))\n",
    "    sorted_data = data.sort_values([\"Filename\",'Points'])\n",
    "    samples_list = list(sorted_data[\"Filename\"].unique())\n",
    "    selected_features = sorted_data\n",
    "    \n",
    "#     stable_subset = selected_features[selected_features['IS_UNSTABLE'] == 0]\n",
    "#     stable_samples = list((stable_subset['Filename']).unique())\n",
    "\n",
    "\n",
    "#     unstable_subset = selected_features[selected_features['IS_UNSTABLE'] == 1]\n",
    "#     unstable_samples = list((unstable_subset['Filename']).unique())\n",
    "\n",
    "#     splitter = GroupShuffleSplit(\n",
    "#         test_size=.10,\n",
    "#         n_splits=2,\n",
    "#         random_state=random_state,\n",
    "#     )\n",
    "#     split = splitter.split(selected_features, groups=selected_features['Filename'])\n",
    "#     train_inds, test_inds = next(split)\n",
    "\n",
    "#     _training_dataframe = selected_features.iloc[train_inds]#CHANGED JT\n",
    "#     testing_dataframe = selected_features.iloc[test_inds]\n",
    "\n",
    "# #     Training & Validation \n",
    "\n",
    "#     splitter = GroupShuffleSplit(\n",
    "#         test_size=.10,\n",
    "#         n_splits=2,\n",
    "#         random_state=random_state,\n",
    "#     )\n",
    "#     split = splitter.split(_training_dataframe, groups=_training_dataframe['Filename'])\n",
    "#     train_inds, val_inds = next(split)\n",
    "\n",
    "#     training_dataframe = _training_dataframe.iloc[train_inds]\n",
    "#     validation_dataframe = _training_dataframe.iloc[val_inds]\n",
    "\n",
    "#     #RESETTING FOR LATER MERGE METADATA WITH THE ACTUAL TESTS\n",
    "#     training_dataframe=training_dataframe.reset_index()\n",
    "#     validation_dataframe=validation_dataframe.reset_index()\n",
    "#     testing_dataframe=testing_dataframe.reset_index()\n",
    "\n",
    "    \n",
    "#     ## Indices \"Datapoints\"\n",
    "\n",
    "#     _training_dataframe=[]\n",
    "#     for i in training_dataframe['Filename'].unique():\n",
    "#         _training_dataframe.append((\"training_dataframe\",i, training_dataframe[training_dataframe['Filename']==i].index.min(), training_dataframe[training_dataframe['Filename']==i].index.max()))\n",
    "\n",
    "#     print(\"#######\")\n",
    "#     _validation_dataframe=[]\n",
    "#     for i in validation_dataframe['Filename'].unique():\n",
    "#         _validation_dataframe.append((\"validation_dataframe\",i, validation_dataframe[validation_dataframe['Filename']==i].index.min(), validation_dataframe[validation_dataframe['Filename']==i].index.max()))\n",
    "\n",
    "#     print(\"#######\")\n",
    "#     _testing_dataframe=[]\n",
    "#     for i in testing_dataframe['Filename'].unique():\n",
    "#         _testing_dataframe.append((\"testing_dataframe\",i, testing_dataframe[testing_dataframe['Filename']==i].index.min(), testing_dataframe[testing_dataframe['Filename']==i].index.max()))\n",
    "\n",
    "\n",
    "#     metadata=pd.concat([pd.DataFrame(_training_dataframe,columns=['Type','Filename','ini_index','end_index']),\n",
    "#                        pd.DataFrame(_validation_dataframe,columns=['Type','Filename','ini_index','end_index']),\n",
    "#                        pd.DataFrame(_testing_dataframe,columns=['Type','Filename','ini_index','end_index'])]\n",
    "#                       )\n",
    "\n",
    "        \n",
    "#     #JOIN TRAINING AND VALIDATION!\n",
    "#     training_dataframe=pd.concat([training_dataframe,validation_dataframe])\n",
    "\n",
    "#     training_x = training_dataframe.iloc[:,1:-2]\n",
    "#     training_y = training_dataframe.iloc[:,-4:-2]\n",
    "    \n",
    "#     validation_x = validation_dataframe.iloc[:,1:-2]\n",
    "#     validation_y = validation_dataframe.iloc[:,-4:-2]\n",
    "\n",
    "#     testing_x = testing_dataframe.iloc[:,1:-2]\n",
    "#     testing_y = testing_dataframe.iloc[:,-4:-2]\n",
    "    \n",
    "\n",
    "#     #to remove the filename\n",
    "#     X_train, y_train = training_x.iloc[:,0:-1],training_y\n",
    "#     X_vals, y_vals = validation_x.iloc[:,0:-1],validation_y\n",
    "#     X_test,y_test = testing_x.iloc[:,0:-1],testing_y\n",
    "    \n",
    "\n",
    "\n",
    "    #NEW AGGREGATION TECHNIQUE NOW I PASS ALL SPLITS!!!! TRAIN,VALS,TEST AS DATAFRAMES!!!!\n",
    "    lst_accu_stratified = cross_validation_function(selected_features,utc_time,random_state)\n",
    "    print(lst_accu_stratified)\n",
    "\n",
    "\n",
    "    #HERE!!!!!\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     Q_y_test=[]\n",
    "#     Q_y_pred=[]\n",
    "#     objective=[]\n",
    "    \n",
    "#     print(\"CHECK POINT!!!!!!!!!!!!!!!!!!!\")\n",
    "#     print(lst_accu_stratified) \n",
    "#     for i in lst_accu_stratified:\n",
    "\n",
    "#         objective.append(f1_score(i[0], i[1],average='macro'))\n",
    "#         print(classification_report(i[0], i[1]))\n",
    "\n",
    "#     print(\"Objective\",objective)\n",
    "\n",
    "\n",
    "#     Q_y_test,Q_y_pred,Q_stoped_window,Q_total_windows,Q_test_name=sk_agregated_score(model, X_test, y_test,metadata,\"testing_dataframe\",  THRESHOLD, RATIO_THRESHOLD)\n",
    "\n",
    "     \n",
    "    return \"DONE\"\n",
    "# objective,model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#OPTIMIZATION PARAMETERS #NOW 200 COMBINATIONS!\n",
    "PREFIXES = [['data/Demo Tool Makino/']]\n",
    "FOLDS=[3] #KFOLD NUMBER OF FOLDS\n",
    "THRESHOLD=[i for i in np.arange(0.01,0.85,0.1)] #[0.2,0.3,0.4,0.5,0.6,0.7,0.8] #FROM 0.1111 UP TO 0 0.9999\n",
    "RATIO_THRESHOLD=[1.5,2,2.5] #HIGHER OR EQUAL THAN 1\n",
    "seq_length=100\n",
    "JUMPING_STEP=80\n",
    "RNDSEED=[39]\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Any\n",
    "from __future__ import annotations\n",
    "from itertools import product\n",
    "\n",
    "def grid_parameters(parameters: dict[str, Iterable[Any]]) -> Iterable[dict[str, Any]]:\n",
    "    for params in product(*parameters.values()):\n",
    "        yield dict(zip(parameters.keys(), params))\n",
    "\n",
    "\n",
    "parameters ={\"FOLDS\":FOLDS,\"THRESHOLD\":THRESHOLD,\"RATIO_THRESHOLD\":RATIO_THRESHOLD,\"PREFIXES\":PREFIXES,\"random_state\":RNDSEED}\n",
    "rfcs=[]\n",
    "objectives=[]\n",
    "for settings in grid_parameters(parameters):\n",
    "    print(settings)\n",
    "    objective2,model2 = experiment(**settings)\n",
    "    print(\"MODEL APPENDED\")\n",
    "    rfcs.append(model2)\n",
    "    objectives.append(np.mean(objective2))\n",
    "    print(\"NOTE RESUTLTS: \", np.mean(objective2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
